{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import graphviz\n",
    "from sklearn import datasets\n",
    "from sklearn import preprocessing\n",
    "from sklearn import tree\n",
    "\n",
    "# Code has NOT been checked for PEP8 yet.\n",
    "\n",
    "# Reading the contents of the iris dataset for testing purposes.\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# Number of perturbed samples to be generated.\n",
    "n = 10000\n",
    "# Number of bins for the histograms of continous attributes.\n",
    "num_bins = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting a decision tree model to the iris dataset.\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(iris.data, iris.target)\n",
    "dot_data = tree.export_graphviz(clf, out_file=None,\n",
    "                                feature_names=iris.feature_names,\n",
    "                                class_names=iris.target_names,\n",
    "                                filled=True, rounded=True,\n",
    "                                special_characters=True)\n",
    "graph = graphviz.Source(dot_data)\n",
    "# graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################\n",
    "# Functions used to generate perturbed samples.\n",
    "\n",
    "\n",
    "def discrete_bucketize(np_vector):\n",
    "    \"\"\"\n",
    "    Estimates histogram buckets for a discrete valued dataset.\n",
    "\n",
    "    Input is the vector, np_vector, of discrete values that is to\n",
    "    be bucketized.\n",
    "    Outputs are two vectors. The first, named values, contains the\n",
    "    normalized version of the unique values found in np_vector.\n",
    "    It is normalized to mean zero and unit variance.\n",
    "    The second output vector, named multinom_rand, contains the\n",
    "    multinomial probability distribution for the elements of the\n",
    "    values vector.\n",
    "    \"\"\"\n",
    "\n",
    "    values, freqs = np.unique(np_vector, return_counts=True)\n",
    "    freqs = freqs/np.sum(freqs)\n",
    "    values = values.astype(float)\n",
    "    values = preprocessing.scale(values)\n",
    "    multinom_rand = np.random.multinomial(n, freqs, 1)[0]\n",
    "    return(values, multinom_rand)\n",
    "\n",
    "\n",
    "def continuous_bucketize(num_bins, np_vector):\n",
    "    \"\"\"\n",
    "    Estimates histogram buckets for a continous valued dataset.\n",
    "\n",
    "    Inputs are the desired number of bins for the histogram,\n",
    "    num_bins, and np_vector, the vector of continous values that are\n",
    "    to be bucketized.\n",
    "    Outputs are two vectors. The first, named h_bins, is a vector of\n",
    "    length num_bins + 1 that lists the values corresponding to the\n",
    "    bin edges in the histogram, after they are normalized to mean\n",
    "    zero and unit variance. The second output vector, named freqs,\n",
    "    contains the probabilities that a randomly generated element will\n",
    "    belong in each of the histogram's bins.\n",
    "    \"\"\"\n",
    "\n",
    "    np_vector = preprocessing.scale(np_vector)\n",
    "    freqs, h_bins = np.histogramdd(np_vector, bins=num_bins)\n",
    "    freqs = freqs/np.sum(freqs)\n",
    "    h_bins = np.asarray(h_bins[0])\n",
    "    return(h_bins, freqs)\n",
    "\n",
    "\n",
    "def discrete_rand_samples(n, values, multinom_rand):\n",
    "    \"\"\"\n",
    "    Generates random values with multinomial probability distribution.\n",
    "\n",
    "    Inputs are n, the number of random numbers that are to be\n",
    "    generated, the vector values that lists all possible values that\n",
    "    n can have, and multinom_rand which contains the multinomial\n",
    "    probability distribution that corresponds to each element in the\n",
    "    values vector. Output is the vector rand that contains n numbers\n",
    "    chosen at random, with replacement, from the elements in the\n",
    "    values vector, following the multinomial probability distribution\n",
    "    that was provided.\n",
    "    \"\"\"\n",
    "\n",
    "    rand = np.zeros(n)\n",
    "    k = 0\n",
    "    for j in range(0, len(values)):\n",
    "        rand[k:k+multinom_rand[j]] = values[j]\n",
    "        k = k + multinom_rand[j]\n",
    "    return(rand)\n",
    "\n",
    "\n",
    "def continuous_rand_samples(n, bins, freqs):\n",
    "    \"\"\"\n",
    "    Generates random values with distribution provided as input.\n",
    "\n",
    "    Inputs are n, the number of random numbers that are to be\n",
    "    generated, the vector bins that lists the values the bin edges\n",
    "    of the histogram, and the vector freqs that lists the probability\n",
    "    that a value that is randomly generated will be contained by the\n",
    "    corresponding histogram bin. Each random number that is generated\n",
    "    is chosen from within a uniform probability distribution with end\n",
    "    values equal to those of a given histogram bin. Output is the\n",
    "    vector tot_samples that contains n random elements chosen as\n",
    "    described above.\n",
    "    \"\"\"\n",
    "\n",
    "    tot_samples = np.zeros(1)\n",
    "    samples_bins = np.random.multinomial(n, freqs, 1)\n",
    "    for j in range(0, len(freqs)):\n",
    "        samples = np.random.uniform(bins[j], bins[j+1], samples_bins[0][j])\n",
    "        tot_samples = np.hstack((tot_samples, samples))\n",
    "    tot_samples = tot_samples[1:, ]\n",
    "    return(tot_samples)\n",
    "\n",
    "#####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is just a test of the discrete bucketizing and random sample\n",
    "# generation. Not really necessary for the example.\n",
    "array = iris.target\n",
    "values, multinom_rand = discrete_bucketize(array)\n",
    "output = discrete_rand_samples(n, values, multinom_rand)\n",
    "\n",
    "# This code uses the iris dataset to evaluate the distributions of\n",
    "# each attribute and generate the perturbed samples.\n",
    "perturbed_samples = np.zeros(n)\n",
    "for j in range(0, iris.data.shape[1]):\n",
    "    array = iris.data[:, j]\n",
    "    h_bins, freqs = continuous_bucketize(num_bins, array)\n",
    "    output = continuous_rand_samples(n, h_bins, freqs)\n",
    "    perturbed_samples = np.vstack((perturbed_samples, output))\n",
    "perturbed_samples = np.transpose(perturbed_samples[1:, ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once perturbed samples have been generated, the line of code below\n",
    "# uses the decision tree we fitted earlier, to get a predicted\n",
    "# classification for each of our perturbed samples.\n",
    "class_perturb_samples = clf.predict(perturbed_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the training dataset for the ML model, we select an input\n",
    "# instance at random, so that we can then use LIME to explain its\n",
    "# output.\n",
    "inst_num = np.round(np.random.uniform(0, iris.data.shape[0], 1))\n",
    "inst_num = inst_num[0].astype(int)\n",
    "x = iris.data[inst_num, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00794906, 0.00844737, 0.00799224, ..., 0.37953495, 0.39249118,\n",
       "       0.38189492])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Computing the LIME weights (Not finished, work in progress)\n",
    "sigma = np.std(np.sum((perturbed_samples - x)**2, axis=1))\n",
    "weights = np.exp(-np.sum((perturbed_samples - x)**2, axis=1) / sigma)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99981604, 0.99834557, 0.99520183, 0.99040066])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ignore this code. Used for testing purposes.\n",
    "x = np.array([[1,2,3,4],[5,6,7,8],[9,10,11,12],[13,14,15,16]])\n",
    "y = ([1,1,1,1])\n",
    "sigma = np.var(np.sum((x-y)**2, axis=1))\n",
    "np.exp(-np.sum((x-y)**2, axis=1)/sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
