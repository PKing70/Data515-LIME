{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import graphviz\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn import preprocessing\n",
    "from sklearn import tree\n",
    "from sklearn import linear_model\n",
    "\n",
    "# Reading the contents of the iris dataset for testing purposes.\n",
    "data_set = datasets.load_iris()\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "# Number of perturbed samples to be generated.\n",
    "n = 10000\n",
    "# Number of bins for the histograms of continous attributes.\n",
    "num_bins = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discrete_rand_samples(n, np_vector):\n",
    "    \"\"\"\n",
    "    Generates discrete random values with desired probability distrib.\n",
    "\n",
    "    Inputs are:\n",
    "    - Desired number of random samples, n.\n",
    "    - The vector of discrete values that describe the distribution\n",
    "      of the random samples that are to be generated, np_vector.\n",
    "    \n",
    "    Output is the vector rand that contains n numbers, chosen at \n",
    "    random, with replacement, from those in the values vector. \n",
    "    Samples follow the multinomial probability distribution\n",
    "    described by multinom_rand.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract the list of unique values in np_vector, and the\n",
    "    # frequency of each value.\n",
    "    values, freqs = np.unique(np_vector, return_counts=True)\n",
    "    freqs = freqs/np.sum(freqs)\n",
    "    values = values.astype(float)\n",
    "    \n",
    "    # Normalize values to mean zero and unit variance.\n",
    "    values = preprocessing.scale(values)\n",
    "    \n",
    "    # Using a multinomial distribution, determine the number\n",
    "    # of instances of each class that are to be generated.\n",
    "    multinom_rand = np.random.multinomial(n, freqs, 1)[0]\n",
    "    \n",
    "    # rand will contain the list of instances that are generated\n",
    "    # based on the numbers in multinom_rand.\n",
    "    rand = np.zeros(n)\n",
    "    k = 0\n",
    "    for j in range(0, len(values)):\n",
    "        rand[k:k+multinom_rand[j]] = values[j]\n",
    "        k = k + multinom_rand[j]\n",
    "    return(rand)\n",
    "\n",
    "\n",
    "def continuous_rand_samples(n, num_bins, np_vector):\n",
    "    \"\"\"\n",
    "    Generates n random values with distribution provided as input.\n",
    "\n",
    "    Inputs are:\n",
    "    - Desired number random samples, n.\n",
    "    - Desired number of bins for the histogram, num_bins.\n",
    "    - The vector of continous values that describe the distribution\n",
    "      of the random samples that are to be generated, np_vector.\n",
    " \n",
    "    The domain of values in np_vector is broken down into num_bins\n",
    "    buckets, and the frequency of samples for each bin is computed.\n",
    "    The frequency determines the number of random samples to be \n",
    "    generated for each bucket, and each random sample generated is\n",
    "    chosen from a uniform probability distribution with end values\n",
    "    equal to those of the corresponding bucket. \n",
    "    \n",
    "    Output is the vector tot_samples that contains n random samples\n",
    "    with the appropriate distribution.\n",
    "    \"\"\"\n",
    "\n",
    "    # Normalize data to mean zero and std. dev of one\n",
    "    np_vector = preprocessing.scale(np_vector)\n",
    "    \n",
    "    # Compute frequency of instances in each of the num_bins buckets.\n",
    "    freqs, h_bins = np.histogramdd(np_vector, bins=num_bins)\n",
    "    freqs = freqs/np.sum(freqs)\n",
    "    \n",
    "    # h_bins lists the bin edges in the distribution.\n",
    "    h_bins = np.asarray(h_bins[0])\n",
    "    tot_samples = np.zeros(1)\n",
    "    \n",
    "    # samples_bins dictates how many random instances have\n",
    "    # to be generated in each bin.\n",
    "    samples_bins = np.random.multinomial(n, freqs, 1)\n",
    "    \n",
    "    # The for loop uses a uniform distribution to generate\n",
    "    # the desired number of instances in each bucket of the\n",
    "    # distribution.\n",
    "    for j in range(0, len(freqs)):\n",
    "        samples = np.random.uniform(h_bins[j], h_bins[j+1],\n",
    "                                    samples_bins[0][j])\n",
    "        tot_samples = np.hstack((tot_samples, samples))\n",
    "    tot_samples = tot_samples[1:, ]\n",
    "    return(tot_samples)\n",
    "\n",
    "\n",
    "#def lime_weights(x, perturbed_samples):\n",
    "#    sigma = np.var(np.sum((perturbed_samples - x)**2, axis=1))\n",
    "#    l_weights = np.exp(-np.sum((perturbed_samples - x)**2, axis=1) /\n",
    "#                       sigma)\n",
    "#    return(l_weights)\n",
    "\n",
    "\n",
    "def lime_fit(x, x_class, perturbed_samples, class_perturb_samples):\n",
    "    \"\"\"\n",
    "    Computes LIME linear model coefficients.\n",
    "\n",
    "    Inputs are:\n",
    "    - x, the instance from the original ML model we are trying to\n",
    "      explain.\n",
    "    - x_class, the classification assigned to x by the original ML\n",
    "      model.\n",
    "    - perturbed_samples which are the random perturbations of inputs\n",
    "      that were generated.\n",
    "    - class_perturb_samples which are the classifications assigned to\n",
    "      each of the perturbations by the original ML model.\n",
    "      \n",
    "    Outputs are:\n",
    "    - Coefficients for the LIME linear model.\n",
    "    - The intercept for the LIME linear model.\n",
    "    - List of LIME weights that were computed for each instance\n",
    "      in perturbed_samples.\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute LIME weights.\n",
    "    sigma = np.var(np.sum((perturbed_samples - x)**2, axis=1))\n",
    "    l_weights = np.exp(-np.sum((perturbed_samples - x)**2, axis=1) /\n",
    "                      sigma)\n",
    "    \n",
    "    #l_weights = lime_weights(x, perturbed_samples)\n",
    "    \n",
    "    # We identify the correct class for the instance we wish to\n",
    "    # interpret, make that class one and all others become\n",
    "    # class zero.\n",
    "    lime_class = class_perturb_samples == x_class\n",
    "    lime_class = lime_class.astype(int)\n",
    "\n",
    "    # Multiply the LIME weights by the perturbed samples and the\n",
    "    # original ML model's output.\n",
    "    perturb_weighted = (perturbed_samples.T * l_weights).T\n",
    "    class_weighted = class_perturb_samples * l_weights\n",
    "\n",
    "    # Using the perturbed samples and the above classification, we fit\n",
    "    # the LIME linear model using LASSO.\n",
    "    # reg = linear_model.LassoCV(eps=0.001, n_alphas=100, cv=5)\n",
    "    reg = linear_model.Lasso(alpha=0.01)\n",
    "    reg.fit(perturb_weighted, class_weighted)\n",
    "    return(reg.coef_, reg.intercept_, l_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucketizing each attribute from the Iris dataset and\n",
    "# generating perturbed samples with the same distribution.\n",
    "perturbed_samples = np.zeros(n)\n",
    "for j in range(0, data_set.data.shape[1]):\n",
    "    array = data_set.data[:, j]\n",
    "    output = continuous_rand_samples(n, num_bins, array)\n",
    "    perturbed_samples = np.vstack((perturbed_samples, output))\n",
    "perturbed_samples = np.transpose(perturbed_samples[1:, ])\n",
    "\n",
    "class_perturb_samples = clf.predict(perturbed_samples)\n",
    "\n",
    "# Selecting the instance to interpret.\n",
    "inst_num = np.round(np.random.uniform(0, data_set.data.shape[0], 1))\n",
    "inst_num = inst_num[0].astype(int)\n",
    "x = data_norm[inst_num, :]\n",
    "x_class = data_set.target[inst_num]\n",
    "\n",
    "# We now fit the LIME linear model to get the coefficients and\n",
    "# intercept, as well as the weight of each random sample, \n",
    "# based on its L2 distance to the instance that is being\n",
    "# interpreted.\n",
    "lime_beta, lime_intercept, lime_weights = lime_fit(x,\n",
    "                                                   x_class,\n",
    "                                                   perturbed_samples,\n",
    "                                                   class_perturb_samples)\n",
    "\n",
    "# Print output of LIME results.\n",
    "print(\"Instance to be interpreted:\")\n",
    "for j in range(0, len(lime_beta)):\n",
    "    print(\"Feature: \", data_set.feature_names[j], \"\\tvalue: \",\n",
    "          data_set.data[inst_num, j], \"\\tnormalized value: \",\n",
    "          data_norm[inst_num, j])\n",
    "print(\"Classification: \",\n",
    "      data_set.target_names[data_set.target[inst_num]],\n",
    "      data_set.target[inst_num])\n",
    "print(\"\\nSignificant coefficients from LIME adjusted linear model:\")\n",
    "for j in range(0, len(lime_beta)):\n",
    "    if(lime_beta[j] != 0):\n",
    "        print(\"Feature: \", data_set.feature_names[j],\n",
    "              \"\\tCoefficient: \", lime_beta[j])\n",
    "print(\"Intercept: \", lime_intercept)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
