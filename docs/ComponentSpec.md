# Data 515 LIME

Spring 2019

__Instructors:__
Joseph L. Hellerstein (jlheller@uw.edu)

David Beck (dacb@uw.edu)

Bernease Herman (bernease@uw.edu)

Sam Gao (gaoz6@cs.washington.edu)

## Component Specification

This component specification covers the proposed final project for the LIME team:

Francisco Javier Salido Magos (javiers@uw.edu)

Patrick King (pking70@uw.edu)

Suman Bhagavathula (sumanbh@uw.edu)

### Software components

Our project has two major modules, each of which can be broken down into multiple components. The two modules are:

- Machine Learning (ML) model that takes a number of predictors as input, including college name, student major, type of school and other, and outputs a likely graduating salary bracket for students from that college/major combination.
- A LIME module that can help a data scientist understand which predictors played a major role in defining the ML model's output for a particular set of inputs.

#### _Machine Learning Model_

For the ML model we will first need to merge, cleanse and prepare our datasets, which will likely require some amount of manual work to identify potential limitations in the datasets themselves, what the best way to encode the data is, and whether value imputation is necessary in some cases. Once the above decisions have been made, we will have to build a component we'll call data_cleanse that automates as many of these decisions as possible.

A set of components will follow the above, that will enable us to train and test various ML models. For this step we plan to leverage functions available in SciKit and other Python packages that enable us to fit various types of potentially complex models, such as boosted trees, random forests, support vector machines with various kernels, and others.

The final component for this first module will be employed to present results from the various approaches we will test, as well as visualizing them. To do this we plan to level components from Python packages such as Matplotlib.

#### _LIME Module_

The LIME module will approximate the underlying ML model with an interpretable one that is learned on randomly perturbed samples of an input/output instance of the underlying model. For purposes of this discussion we shall heretofore refer to the underlying ML model as the "complex" model, and to the LIME model as the "simple" model.

Inputs to the LIME module should be:

- The single instance of predictors, _x_, and corresponding complex model output _f_(_x_) that we wish to interpret.
- A description of the domain and probability distribution for each predictor, encoded in the form of one histogram for each predictor in the model.
- The complex model itself, as we will use it to get labels for a number of perturbed samples that will be generated by the module.

Inputs to the first component, lime_sampler, are the same as those for the entire LIME module, and the output will be a set of _n_ randomized samples of the complex model's behavior. Randomized samples are generated by randomly chosing a value from each predictor/histogram to form a randomized instance. The randomized instance is then submitted to the complex model to get the corresponding label. The number of randomized samples we generate, _n_, should be as large as possible.

The second component, lime_fit, will take the set of randomized samples generated by lime_sampler as input, and output a linear model, the simple model, that describes the behavior of the complex model in the vicinity of the input instance we wish to interpret. To do this, lime_fit computes a proximity measure/weight for each random sample that penalizes it depending on how far appart it is from the instance we wish to interpret. The further appart the sample is from the original instance, the greater the cost of considering it to fit the simple model. Once we have the proximity measure for each sample, we fit a linear model with an added penalty for complexity, represented by the number of non-zero coefficients of the linear model. This is our simple model. To fit the model we must employ shrinkage methods, either ridge regression or LASSO, as these methods shrink the value of the linear regression coefficients for those predictors that are less correlated to the model's output, effectively helping us reduce the complexity of the simple model by reducing the number of predictors it uses.

Our third component, lime_result, will generate a human-readable output that will facilitate interpretation of the complex ML model's behavior in the locality of the selected input instance. For this third component we will most likely leverage ML functions from variouis Python packages, such as Matplotlib.

### Interactions to accomplish use cases

The interaction is quite simple. The first module is the ML model that the data scientist needs to understand. Since the model is complex, direct interpretation of its output would require too much effort or is simply not possible in human terms.

In order to gain insight, the data scientist needs to develop intuition on how the predictors at the input of the model affect its output classification, for each test instance. Given that we are dealing with a complex ML model, we use LIME to provide local explanations that will enable the data scientist to acquire intuition of what drive's the model's decisions in the vicinity of each specific test instance that are to be interpreted.

### Preliminary plan

Our preliminary plan consists of a two-pronged approach:

For module one, the Machine Learning model:

- Inspection of the datasets we plan to employ.
- Implementation of the data_cleanse component.
- Fitting of a number of ML models to classify colleges and majors into potential salary brackets.

For the LIME module:

- Do additional research on some of the mathematical and logistic subtleties of sampling and optimization for LIME.
- Build and test the lime_sampler.
- Build and test the lime_fit.
- Build and test the lime_result.
- Do end-to-end testing of the components.

Once the above steps have been completed, we will evaluate the outputs of the ML model(s) using LIME and produce the final report.
