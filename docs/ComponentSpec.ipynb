{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data 515 LIMEaid\n",
    "_Local Interpretable Model-agnostic Explanations_ and aid to understanding the LIME method to interpretability.\n",
    "\n",
    "Spring 2019\n",
    "\n",
    "__Instructors:__\n",
    "Joseph L. Hellerstein (jlheller@uw.edu)\n",
    "\n",
    "Bernease Herman (bernease@uw.edu)\n",
    "\n",
    "Sam Gao (gaoz6@cs.washington.edu)\n",
    "\n",
    "## Component Specification\n",
    "\n",
    "This component specification covers the proposed final project for the LIME team:\n",
    "\n",
    "Francisco Javier Salido Magos (javiers@uw.edu)\n",
    "\n",
    "Patrick King (pking70@uw.edu)\n",
    "\n",
    "Suman Bhagavathula (sumanbh@uw.edu)\n",
    "\n",
    "For a more general explanation of LIME, as well as links to the code and original paper by LIME's authors, you can go here: https://www.oreilly.com/learning/introduction-to-local-interpretable-model-agnostic-explanations-lime\n",
    "\n",
    "### Software components\n",
    "\n",
    "Our project has three major modules, each of which can be broken down into multiple components. The three modules are:\n",
    "\n",
    "- The datasets module has two sub modules, one for data acquisition and another for clean and merge to get the final dataset used for training and analysis.\n",
    "- Machine Learning (ML) model that takes a number of predictors as input, including college name, student major, type of school and other, and outputs a likely graduating salary bracket for students from that college/major combination.\n",
    "- A LIME module with two sub modules that can help a data scientist understand which predictors played a major role in defining the ML model's output for a particular set of inputs.\n",
    "\n",
    "#### _Data Sets_\n",
    "\n",
    "The first submodule in the datasets module, named get_college_datasets, contains two functions:\n",
    "\n",
    "- $get\\_most\\_recent\\_cohorts$, which acquires the most recent cohorts dataset. \n",
    "- $get\\_salaries\\_by\\_region$, which acquires the salaries by region dataset. \n",
    "\n",
    "These functions do not take any input. The output will be a pandas dataframe for the corresponding dataset loaded from the data directory. \n",
    "\n",
    "The second submodule, named load_college_dataset, that normalizes the College names columns in both the datasets, joins them based on these normalized college names, retains a set of hand selected features, generates a new column that has Median mid career salary categorized into three classes to be used as the Classification target. The implementation consists of two functions:\n",
    "\n",
    "- $load\\_college\\_dataset$, that invokes the cleaning function to get the final prepared pandas dataset to pass it on to the caller.\n",
    "- $clean\\_and\\_merge\\_college\\_datasets$, that cleans and merges the data sets.\n",
    "\n",
    "#### _Machine Learning Model_\n",
    "\n",
    "The machine learning module, called fit_sklearn_models, contains three functions, each of which enables us to train and test a specific scikitlearn ML model. The three functions fitting the models are: \n",
    "\n",
    "- $fit\\_multiclass\\_logistic\\_regression$\n",
    "- $fit\\_random\\_forest$\n",
    "- $fit\\_decision\\_tree$\n",
    "\n",
    "Each loads the dataset using the functions in the get_college_datasets module, scales the inputs to values between 0 and 1, splits the dataset into training and testing and then fits the corresponding machine learning algorithm. They take one optional parameter called print_score that can be used to specify whether or not to print the testing accuracy score after model training and using the predict function on the test set. The output of the function is the model object itself.\n",
    "\n",
    "#### _LIME Module_\n",
    "\n",
    "The first submodule in the LIME module, called LIMEaid, will approximate the underlying ML model with an interpretable linear regression that is learned on randomly perturbed samples of an input/output instance of the underlying model. For purposes of this discussion we shall heretofore refer to the underlying ML model as the \"complex\" model, and to the LIME model as the LIME model.\n",
    "\n",
    "The main idea of LIME is that we have some complex classifier, a random forest, boosted trees or a neural network, the workings of which cannot be explained in terms a human being can easily understand. We say that the complex ML model $f(x)$ is not _interpretable_ if, given a set of input attributes $x = (x_1, x_2, ...., x_m)$, an output $f(x)$ that is either a value indicating the probability that $x$ belongs to a certain class, or a label designating that class, the reason why a particular $x_0$ results in a particular output $f(x_0)$ cannot be understood by a person, even if all the details of the model are known. Since the global workings of the complex model cannot be explained in human terms, we use simpler linear models, LIME models, to explain its local behavior in the vicinity of our chosen $(x, f(x))$.\n",
    "\n",
    "For purposes of this project we will limit the discussion to Machine Learning models for which the individual input attributes can be understood by humans, they are not engineered features, and that can be represented in tabular form.\n",
    "\n",
    "Inputs to the LIME module should be:\n",
    "\n",
    "- The single instance $x$, and corresponding complex model output $f(x)$ that we wish to interpret.\n",
    "- A description of the domain and probability distribution for each normalized predictor (mean $m = 0$ and standard deviation $\\sigma = 1$), encoded in the form of one histogram for each predictor in the model.\n",
    "- Access to the complex model itself, which we will use to compute labels for a number of perturbed samples that will be generated by the module.\n",
    "\n",
    "The LIMEaid two functions:\n",
    "\n",
    "- $lime\\_sample$ will generate random samples or perturbations that will simulate real input data. Inputs to this function will be the number of samples we wish to generate and the data used to train the complex ML model. Training data is required so that the probability distribution of random samples will match that of the actual data. The output will be a set of $n$ randomized samples of the complex model's behavior. Samples are generated by randomly chosing a value from each predictor/histogram, to form a randomized instance $z_i = (z_{i,1}, z_{i,2}, ...., z_{i,m})$, which is then provided as input to the complex model to obtain the corresponding output $f(z_i)$. The total number of randomized samples we generate, $n$, should be large, and these samples will be distributed all over the sample space. Note that we'll need two options when running this function, one that will generate random samples for attributes that have discrete values, and a second for continous values.\n",
    "\n",
    "- $lime\\_fit$, will take the set of randomized samples generated by $lime\\_sample$ as input, and output a linear model, the LIME model, that describes the behavior of the complex model in the vicinity of the input instance $x$. To do this, $lime\\_fit$ will need to execute two steps:\n",
    "\n",
    "1.- Compute a proximity measure/weight for each random sample. Since $lime\\_fit$ produces a set of randomized samples that are distributed across the entire sample space, we'll use this proximity measure $\\pi_x(z_i)$ to penalize the samples based on how distant they are from $x$. The further away $z_i$ is from $x$, the greater the penalty:\n",
    "\n",
    "$$\\pi_x(z_i) = \\exp^{-\\frac{D(x,z_i)^2}{\\sigma^2}}$$ \n",
    "\n",
    "where $D(x,z_i)$ is the L2 distance: \n",
    "\n",
    "$$D(x,z_i)^2 = (x_1 - z_{i,1})^2 + (x_2 - z_{i,2})^2 + .... + (x_m - z_{i,m})^2$$\n",
    "\n",
    "2.- Fit a linear model using the LASSO algorithm, with $x$ and the set of perturbed samples $z_1, z_2, ...., z_n$ as inputs. Thus, we wish to find:\n",
    "\n",
    "$$arg min_g \\sum_{i} \\pi_x(z_i)(f(z_i) - wz_i)^2 + \\lambda|w|$$ \n",
    "\n",
    "where: \n",
    "\n",
    "$$g(z_i) = w_1 z_{i,1} + w_2z_{i,2} + .... + w_mz_{i,m}$$\n",
    "\n",
    "and $w$ is the set of coefficients $w_1, w_2, ...., w_m$ assigned by the linear model to each predictor.\n",
    "\n",
    "To fit the model we must employ the shrinkage method LASSO, as it shrinks the magnitude of those linear regression coefficients $w$ that are less correlated to the model's output, effectively helping us reduce the complexity of the simple model by reducing the number of non-zero predictors it uses.\n",
    "\n",
    "The output will be the intercept and coefficients of the linear model fitted using LASSO. This LIME model will facilitate interpretation of the complex ML model's behavior in the locality of the selected input instance.\n",
    "\n",
    "The second submodule in the LIME module is called LIMEdisplay, which contains a single function: \n",
    "\n",
    "- $lime\\_display$ that will take the instance we are trying to explain, the random samples that were generated by $lime\\_sample$, and the output of $lime\\_fit$, and display one or two graphs illustrating the results. The first graph will only be displayed if LIME determines that there are only two attributes that are significant to the classification. The second graph will be displayed always, and it is a representation of the values resulting from computing the LIME regression function on the values of each random sample, plus the instance that is being explained, displayed by class. Note that the output of this function is limited to cases in which we have up to six different classes in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LIMEaid component details](images/Components.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MVC architecture\n",
    "\n",
    "The modules mentioned in the above section are organized into a Model View Controller architecture, with a folder for each of these components. As the name suggests, the Model contains all of the modules and functions for data acquisition and cleaning, the Controller contains modules and functions that perform various business operations on the data and the View contains all of the modules and functions for presenting the data and analysis results. The structure is shown below for the current project:\n",
    "\n",
    "Model:\n",
    "- get_college_datasets (return the individual datasets)\n",
    "- load_college_datasets (clean and join the datasets, return the final dataset ready for processing)\n",
    "\n",
    "Controller:\n",
    "- fit_sklearn_models (apply the three different scikitlearn models on the dataset and return the model object)\n",
    "- LIMEaid (contains the custom implementations for LIME functionality)\n",
    "\n",
    "View:\n",
    "- LIMEdisplay (provides the means to display a graphical representation of the results obtained by LIMEaid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactions to accomplish use cases\n",
    "\n",
    "The first \"component\" is the dataset for which there is a model that the data scientist wants to explain, whether using our example College data, Iris, or any other compatible data set.\n",
    "\n",
    "The data supported by LIMEaid is tabular data of numeric features that can be classified by any Scikit-learn classifier. The flowchart below shows the key pseudocode calls that cause the components of LIMEaid to interact to accomplish use cases.\n",
    "\n",
    "1. First, data is provided as represented by the __Data__ step, which uses Scikit-learn's Iris data in pseudocode.\n",
    "2. This data is used to train a model, such as a decision tree, as shown in the __Train model__ step. Sklearn returns a trained classifier object as clf.\n",
    "3. This same data used to train the model in step 2 is also used to create a sample of \"perturbed data\" in the __Perturb data__ step. This produces pert, which represents a dataset that is similar but different to the original dataset, with the number of simulated, perturbed samples represented by n.\n",
    "4. pert, the perturbed data sample, is passed to the predict method of clf to produce the classifications for each instance of perturbed data, returned to class_pert. This is the __Classify perturbed data__ step.\n",
    "5. The data scientist chooses which instance of actual data to LIME analyze, as represented by x (the features) and x_class, the classification of this instance of data according to the scientist's model. This is represented in the __Choose Instance__ step.\n",
    "6. In the __LIMEaid model__ step, the perturbed data (pert), the perturned classifications (class_pert), the instance of actual data of interest (x), and the classification of the instance (x_class) are passed as arguments to lime_fit. This produces the result of the most significant features used by the model for this classification, according to LIME methods described earlier.\n",
    "7. In the __LIMEaid explanation__ step, this result is passed to lime_display which produces easy-to-explain lists of most significant coefficients and plots of regression to explain the model's classification of x.\n",
    "\n",
    "For use scenario #1 (Model verification), a data scientist should pass their training dataset in steps 3 and 4 above, in order to consider whether the model seems to be doing what the scientist expects or wants, or whether to refine the model for better results or to avoid untrustworthy or dubious classifications.\n",
    "\n",
    "For use scenario #2 (Decision explanation), a data scientist should pass their actual, \"real-world\" data in steps 3 and 4 above, to be able to accurately evaluate and explain how a model's decision seems to have been made.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LIMEaid component interaction](images/Limeaid.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project plan\n",
    "\n",
    "Our plan consists of a two-pronged approach:\n",
    "\n",
    "The Data and Machine Learning modules:\n",
    "\n",
    "- Inspection of the datasets we plan to employ.\n",
    "- Implementation of the get_college_datasets component.\n",
    "- Fitting of a number of ML models to classify colleges and majors into potential salary brackets.\n",
    "\n",
    "For the LIME module:\n",
    "\n",
    "- Do additional research on some of the mathematical and logistic subtleties of sampling and optimization for LIME.\n",
    "- Build and test $lime\\_sample$.\n",
    "- Build and test $lime\\_fit$.\n",
    "- Build and test $lime\\_display$.\n",
    "- Do end-to-end testing of the components.\n",
    "\n",
    "Once the above steps have been completed, we have evaluated the outputs of the ML model(s) using LIME and produced final reports.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
