{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIME example Iris dataset.\n",
    "\n",
    "For this example we employ the Iris dataset that provides four attributes that can be used to classify flowers into three classes. \n",
    "\n",
    "Using this dataset we fit a Decision Tree as our original ML model, we then generate a number of random instances with the four attributes and use the Decision Tree to classify them. We then use LIME to fit a regression model that identifies which of the four attributes have the greatest impact in the model's classifications.\n",
    "\n",
    "If we compare the structure of the decision tree to the output from LIME we can appreciate that they yield similar results. LIME concludes that only two of the four attributes play a significant role in the tree's decisions, petal width and length, and from the structure of the tree we can confirm that that is the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'LIMEaid'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-eff1f4f508a1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mLIMEaid\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mla\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgraphviz\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'LIMEaid'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import LIMEaid as la\n",
    "import graphviz\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn import preprocessing\n",
    "from sklearn import tree\n",
    "from sklearn import linear_model\n",
    "\n",
    "# Reading the contents of the iris dataset for testing purposes.\n",
    "data_set = datasets.load_iris()\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "# Number of perturbed samples to be generated.\n",
    "n = 100000\n",
    "# Number of bins for the histograms of continous attributes.\n",
    "num_bins = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning model that is to be interpreted.\n",
    "\n",
    "Use the Iris dataset to fit a model, a Decision Tree, that we'll use later to classify the random samples we generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting a decision tree model to the iris dataset.\n",
    "data_norm = preprocessing.scale(data_set.data)\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(data_norm, data_set.target)\n",
    "dot_data = tree.export_graphviz(clf, out_file=None,\n",
    "                                feature_names=data_set.feature_names,\n",
    "                                class_names=data_set.target_names,\n",
    "                                filled=True, rounded=True,\n",
    "                                special_characters=True)\n",
    "graph = graphviz.Source(dot_data)\n",
    "\n",
    "# graph.render creates a PDF file with a diagram of the structure\n",
    "# of the decision tree. This file is stored in the local directory.\n",
    "# graph.render(\"iris\")\n",
    "\n",
    "# graph creates a diagram of the structure of a decision tree and\n",
    "# displays it in the notebook.\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that most nodes in the decision tree will do their classification based on either petal length or petal width. Only 3 of the 150 samples are classified based on sepal length, and 3 based on sepal width."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting the output of the Decision Tree using LIME.\n",
    "\n",
    "We now take the 150 instances of the Iris dataset, attributes only, and form them we generate n random samples with similar distributions in the values of each attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucketizing each attribute from the Iris dataset and\n",
    "# generating perturbed samples with the same distribution.\n",
    "perturbed_samples = np.zeros(n)\n",
    "for j in range(0, data_set.data.shape[1]):\n",
    "    array = data_set.data[:, j]\n",
    "    output = la.lime_sample(n, True, array, num_bins)\n",
    "    perturbed_samples = np.vstack((perturbed_samples, output))\n",
    "perturbed_samples = np.transpose(perturbed_samples[1:, ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once random samples with the right distributions for each attribute have been generated, we provide them as input to the Decision Tree we fitted earlier, and obtain the classification for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_perturb_samples = clf.predict(perturbed_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select a single instance from the Iris dataset. This is the instance we will try to interpret using LIME."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the instance to interpret.\n",
    "inst_num = np.round(np.random.uniform(0, data_set.data.shape[0], 1))\n",
    "inst_num = inst_num[0].astype(int)\n",
    "# x is the selected instance, and x_class is the class assigned\n",
    "# by the decision tree. \n",
    "x = data_norm[inst_num, :]\n",
    "x_class = data_set.target[inst_num]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calling LIME function to get intepretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now fit the LIME linear model to get the coefficients and\n",
    "# intercept, as well as the weight of each random sample, \n",
    "# based on its L2 distance to the instance that is being\n",
    "# interpreted.\n",
    "lime_beta, lime_int, lime_weigh = lime_fit(x,\n",
    "                                           x_class,\n",
    "                                           perturbed_samples,\n",
    "                                           class_perturb_samples)\n",
    "\n",
    "# Print output of LIME results.\n",
    "print(\"Instance to be interpreted:\")\n",
    "for j in range(0, len(lime_beta)):\n",
    "    print(\"Feature: \", data_set.feature_names[j], \"\\tvalue: \",\n",
    "          data_set.data[inst_num, j], \"\\tnormalized value: \",\n",
    "          data_norm[inst_num, j])\n",
    "print(\"Classification: \",\n",
    "      data_set.target_names[data_set.target[inst_num]],\n",
    "      data_set.target[inst_num])\n",
    "print(\"\\nSignificant coefficients from LIME adjusted\"\n",
    "      \" linear model:\")\n",
    "for j in range(0, len(lime_beta)):\n",
    "    if(lime_beta[j] != 0):\n",
    "        print(\"Feature: \", data_set.feature_names[j],\n",
    "              \"\\tCoefficient: \", lime_beta[j])\n",
    "print(\"Intercept: \", lime_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intepreting the results\n",
    "\n",
    "We selected the Decision Tree for this example because it provides an output that is interpretable; the structure of the tree itself. An examination of this structure reveals that in almost all cases, branching decisions are made based on either petal length or petal width, which is what our LIME linear model is indicating, as the LIME coefficients for sepal length and sepal width are zero.\n",
    "\n",
    "Below we plot the random samples and the instance that is being interpreted on the petal length vs. petal width plane and, as we can observe, classes can be separated for the most part by looking at these two attributes. Note that those random samples that were classificated differently from the instance we are interpreting are merged into a single out-of-class representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For visualization we separate perturbed samples classified\n",
    "# by the decision tree as in-class, from those classified\n",
    "# out-of-class.\n",
    "full_data = np.column_stack((perturbed_samples,\n",
    "                             class_perturb_samples))\n",
    "in_class_data = full_data[full_data[:, 4] ==\n",
    "                          data_set.target[inst_num]]\n",
    "out_class_data = full_data[full_data[:, 4] !=\n",
    "                           data_set.target[inst_num]]\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10.0, 10.0]\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.scatter(in_class_data[:, 2], in_class_data[:, 3], marker='x',\n",
    "           c='b', label='In class')\n",
    "ax.scatter(out_class_data[:, 2], out_class_data[:, 3], marker='+',\n",
    "           c='c', label='Out of class')\n",
    "ax.scatter(data_norm[inst_num, 2], data_norm[inst_num, 3],\n",
    "           marker='o', c='r', label='Instance')\n",
    "ticksx = ax.set_xticks([-2, -1.5, -1, -1.5, -0.5, 0, 0.5, 1, 1.5, 2])\n",
    "ticksy = ax.set_yticks([-2, -1.5, -1, -1.5, -0.5, 0, 0.5, 1, 1.5, 2])\n",
    "plt.grid(b=True, which='both', color='0.85', linestyle='-')\n",
    "ax.set_xlabel('Random Petal Length')\n",
    "ax.set_ylabel('Random Petal Width')\n",
    "plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we separate all instances by how they were classified by the Decison Tree and compute the value of the LIME regression for each instance, as well as that of the instance we are explaining and plot the results.\n",
    "\n",
    "We can see that the output of our LIME regression model separates all three classes for the most part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute all LIME regression values for all perturbed samples\n",
    "# and for the instance to be explained.\n",
    "zero_class_reg = full_data[full_data[:, 4] == 0]\n",
    "zero_class_reg = (lime_beta[2]*zero_class_reg[:, 2] +\n",
    "                  lime_beta[3]*zero_class_reg[:, 3] + lime_intercept)\n",
    "one_class_reg = full_data[full_data[:, 4] == 1]\n",
    "one_class_reg = (lime_beta[2]*one_class_reg[:, 2] +\n",
    "                 lime_beta[3]*one_class_reg[:, 3] + lime_intercept)\n",
    "two_class_reg = full_data[full_data[:, 4] == 2]\n",
    "two_class_reg = (lime_beta[2]*two_class_reg[:, 2] +\n",
    "                 lime_beta[3]*two_class_reg[:, 3] + lime_intercept)\n",
    "instance = (lime_beta[2]*data_norm[inst_num, 2] +\n",
    "            lime_beta[3]*data_norm[inst_num, 3] + lime_intercept)\n",
    "\n",
    "# Plot all LIME regression values for perturbed samples\n",
    "# against their classificaton by the decision tree.\n",
    "plt.rcParams['figure.figsize'] = [5.0, 10.0]\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.scatter(np.zeros(len(zero_class_reg)), zero_class_reg,\n",
    "           marker='o', c='c')\n",
    "ax.scatter(np.zeros(len(one_class_reg))+1, one_class_reg,\n",
    "           marker='o', c='b')\n",
    "ax.scatter(np.zeros(len(two_class_reg))+2, two_class_reg,\n",
    "           marker='o', c='k')\n",
    "ax.scatter(data_set.target[inst_num], instance,\n",
    "           marker='o', c='r')\n",
    "ticksx = ax.set_xticks([0, 1, 2])\n",
    "labelsx = ax.set_xticklabels(['Setosa', 'Versicolor', 'Virginica'])\n",
    "ax.set_ylabel('LIME Regression')\n",
    "plt.grid(b=True, which='both', color='0.85', linestyle='-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
